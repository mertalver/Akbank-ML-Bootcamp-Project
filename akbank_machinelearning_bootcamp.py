# -*- coding: utf-8 -*-
"""Akbank-MachineLearning-Bootcamp

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hs4YgbLCkTuwaMEOcXxUTPWI-JS-IAQc

# **1-) Importing Required Libraries**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

"""# **2-) Gathering and Observing Data**"""

dataset = pd.read_csv("Melbourne_housing_FULL.csv")
dataset.head(5)

dataset.shape

num_columns = dataset.shape[1]
print("Number of Columns:", num_columns)

print("Size of Dataset:", dataset.size)

dataset.info()

"""# **3-) Exploratory Data Analysis**"""

# Sütunları kategorik ve sayısal olarak ikiye ayırdık.

categorical_columns = ["Suburb", "Address", "Type", "Method", "SellerG", "Date", "CouncilArea", "Regionname"]
num_columns = ["Rooms", "Price", "Distance", "Postcode", "Bedroom2", "Bathroom", "Car", "Landsize", "BuildingArea", "YearBuilt", "Lattitude", "Longtitude", "Propertycount"]

# Veri setinde eğer varsa tekrar eden verileri sildik.

dataset = dataset.drop_duplicates()

dataset.duplicated().any()

# Veri setindeki eksik değerleri belirtildiği gibi mod ile doldurduk.

for column in categorical_columns:
  dataset[column] = dataset[column].fillna(dataset[column].mode().loc[0])

for column in num_columns:
  dataset[column] = dataset[column].fillna(dataset[column].mode().loc[0])

dataset.info()

# Aykırı değerleri Z-Skor ile tespit edip bunları sildik.

from scipy import stats
z_scores_landsize = np.abs(stats.zscore(dataset['Landsize']))
z_scores_buildingarea = np.abs(stats.zscore(dataset['BuildingArea']))
threshold = 3

outliers_landsize = np.where(z_scores_landsize > threshold)
outliers_buildingarea = np.where(z_scores_buildingarea > threshold)

outliers_indices = set(outliers_landsize[0]) | set(outliers_buildingarea[0])

dataset = dataset.drop(index=list(outliers_indices))

dataset

"""### **3a-) Data Visualization**"""

dataset['Regionname'].unique()

plt.figure(figsize=(10, 8))
sns.histplot(dataset['Price'], kde=True, color='gray', label='All Regions')
plt.title('Price Distributions on All Regions')
plt.xlabel('Price')
plt.ylabel('Count')
plt.legend()
plt.show()

regions = ['Northern Metropolitan', 'Southern Metropolitan', 'Eastern Metropolitan', 'Western Metropolitan', 'South-Eastern Metropolitan', 'Northern Victoria', 'Eastern Victoria', 'Western Victoria']
colors = ['blue', 'red', 'green', 'purple', 'orange', 'cyan', 'darkgreen', 'magenta']

all_regions = dataset['Price']

for region, color in zip(regions, colors):
  plt.figure(figsize=(10, 8))
  region_data = dataset[dataset['Regionname'] == region]['Price']
  sns.histplot(region_data, kde=True, color=color, label=region)
  plt.title('Price Distributions by Region')
  plt.xlabel('Price')
  plt.ylabel('Count')
  plt.legend()
  plt.show()

for i in range(len(num_columns)):
  if num_columns[i] != "Price":
    sns.pairplot(data=dataset, x_vars=[num_columns[i]], y_vars=["Price"],  hue=num_columns[i])
    plt.title(f"Relationship between {num_columns[i]} and Price")
    plt.show()

# Çok fazla değişken türü olduğu için OneHotEncoder yerine LabelEncoder kullandık.

le = LabelEncoder()
dataset = dataset.apply(lambda x: le.fit_transform(x) if x.dtype == "object" else x)

dataset.head(5)

correlation_matrix = dataset.corr()

plt.figure(figsize=(14, 12))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=.5)
plt.title("Correlation Matrix ")

plt.show()

# Model üzerinde olumsuz etkisi olan sütunları çıkardık.

dataset = dataset.drop(["Suburb", "Lattitude"], axis=1)
dataset.nunique()

"""# **4-) Model Selection**"""

X = dataset.drop("Price", axis=1)
y = dataset["Price"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

models = {
    'Lasso': {
        'model': Lasso()
        },
    'LinearRegression': {
        'model': LinearRegression()
        },
    'Ridge': {
        'model': Ridge()
        },
    'ElasticNet': {
        'model': ElasticNet()
        },
    'KNeighborsRegressor': {
        'model': KNeighborsRegressor()
        },
    'RandomForestRegressor': {
        'model': RandomForestRegressor()
        },
    'GradientBoostingRegressor': {
        'model': GradientBoostingRegressor()
        },
    'AdaBoostRegressor': {
        'model': AdaBoostRegressor(n_estimators = 5, learning_rate = 1.2, loss = 'exponential', random_state = 2)
        },
}

for model_name, model_info in models.items():
    model = model_info['model']

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mae = mean_absolute_error(y_test, y_pred)
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)

    print(f"Model: {model_name}")
    print("MAE ", mae)
    print("MSE: ", mse)
    print("RMSE: ", rmse)
    print("R2: ", r2)
    print()

# R2 Skoruna göre en yüksek değeri RandomForestRegressor ile elde ettik ve bu modeli en iyi model olarak seçtik.

best_model = RandomForestRegressor(n_estimators=100, random_state = 0)
best_model.fit(X_train, y_train)

y_pred = best_model.predict(X_test)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("En iyi model: Random Forest Regressor")
print("MAE: ", mae)
print("MSE: ", mse)
print("RMSE: ", rmse)
print("R2: ", r2)